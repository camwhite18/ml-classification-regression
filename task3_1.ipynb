{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import r_regression, SelectKBest\n",
    "\n",
    "# Import the functions created for reusability from the Python file\n",
    "from task3_1 import remove_outliers, remove_no_variance, standardise_features, pearson_correlation_matrix, \\\n",
    "    fill_missing_dates, fill_missing_quarters, fill_missing_days, convert_date_to_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Star Dataset Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the Star Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "star_file_name = 'datasets/star_assessment.csv'\n",
    "star_features = np.genfromtxt(star_file_name, delimiter=',', skip_header=True, encoding=\"utf-8\", usecols=range(0, 17))\n",
    "star_labels = np.genfromtxt(star_file_name, delimiter=',', skip_header=True, encoding=\"utf-8\", usecols=17, dtype=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of features: 16\n",
      "Number of classes: 3\n",
      "Class names: ['GALAXY' 'QSO' 'STAR']\n"
     ]
    }
   ],
   "source": [
    "# Explore the star dataset\n",
    "print(f'Number of samples: {star_features.shape[0]}')\n",
    "print(f'Number of features: {star_features.shape[1]-1}')\n",
    "print(f'Number of classes: {np.unique(star_labels).shape[0]}')\n",
    "print(f'Class names: {np.unique(star_labels)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there are only 3 classes, the star dataset is clearly a classification task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GALAXY': 59445, 'QSO': 18961, 'STAR': 21594}\n"
     ]
    }
   ],
   "source": [
    "# Output the distribution of the classes\n",
    "unique, counts = np.unique(star_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Filling in Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 1: 60 missing values\n",
      "Column 2: 55 missing values\n",
      "Column 3: 65 missing values\n",
      "Column 4: 70 missing values\n",
      "Column 5: 63 missing values\n",
      "Column 6: 50 missing values\n",
      "Column 7: 59 missing values\n",
      "Column 8: 61 missing values\n",
      "Column 9: 59 missing values\n",
      "Column 10: 51 missing values\n",
      "Column 11: 68 missing values\n",
      "Column 12: 59 missing values\n",
      "Column 13: 58 missing values\n",
      "Column 14: 49 missing values\n",
      "Column 15: 50 missing values\n",
      "Column 16: 61 missing values\n",
      "Column 17: 62 missing values\n",
      "Total: 1000 missing values\n"
     ]
    }
   ],
   "source": [
    "# Count the number of missing values in each column\n",
    "missing_vals = np.sum(np.isnan(star_features), axis=0)\n",
    "for k, v in enumerate(missing_vals):\n",
    "    print(f'Column {k+1}: {v} missing values')\n",
    "print(f'Total: {sum(missing_vals)} missing values')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use a KNN imputer to fill in the missing values\n",
    "knn_imputer = KNNImputer()\n",
    "star_features_imputed = knn_imputer.fit_transform(star_features)\n",
    "\n",
    "missing_vals = np.sum(np.isnan(star_features_imputed), axis=0)\n",
    "print(f'Total: {sum(missing_vals)} missing values')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Encoding the Class Labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode the class labels\n",
    "label_encoder = LabelEncoder()\n",
    "star_labels_encoded = label_encoder.fit_transform(star_labels)\n",
    "\n",
    "# Output the mapping between the class names and the encoded values\n",
    "for k, v in enumerate(label_encoder.classes_):\n",
    "    print(f'{v} : {k}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Removing Samples with Outliers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_size_with_outliers = star_features_imputed.shape[0]\n",
    "star_features_without_outliers, star_labels_without_outliers = remove_outliers(star_features_imputed, star_labels_encoded)\n",
    "\n",
    "print(f'Number of samples removed: {sample_size_with_outliers - star_features_without_outliers.shape[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove Features with No Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use a variance threshold to remove any features with zero variance\n",
    "star_features_no_variance_removed, removed_columns = remove_no_variance(star_features_without_outliers)\n",
    "\n",
    "print(f\"Removed column indexes: {removed_columns}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, the `rerun_ID` feature has zero variance, so it is removed.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scale the features to have a mean of 0 and a standard deviation of 1 before feature selection to avoid biasing the feature selection process with high-scale features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "star_features_standardised = standardise_features(star_features_no_variance_removed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection Using the Filter Method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Then use the f_classif method to determine the correlation between each feature and the class labels\n",
    "select_k_best = SelectKBest(k='all')\n",
    "select_k_best.fit(star_features_standardised, star_labels_without_outliers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the sorted ANOVA F-values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(select_k_best.scores_, marker='o')\n",
    "plt.xlabel('Feature index')\n",
    "plt.ylabel('ANOVA F-value')\n",
    "plt.title('ANOVA F-Values for Sorted Features')\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the plot above, we can see that the features with indexes  3 (`u`), 4 (`g`), 5 (`r`), 6 (`i`), 7 (`z`), 11 (`spec_obj_ID`), 12 (`redshift`), 13 (`plate`), and 14 (`MJD`) have significantly higher ANOVA F-values than the other features, thus they have a stronger influence on the target. Additionally, the upon further investigation into what each column represents through visiting [Kaggle](https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17), seemingly these features are the most relevant to the classification of an object as a star, galaxy, or quasar.\n",
    "\n",
    "Therefore, the features that will be dropped are:\n",
    "- `obj_ID`: given that it is just an identifier, it does not provide any useful information for classification and would only add noise to the model.\n",
    "- `alpha`: this feature represents the right ascension of the object, which is a coordinate used to locate objects in the sky. It shouldn't provide any benefit to the model in the classification of an object as a star, galaxy, or quasar as they are distributed across the sky.\n",
    "- `delta`: similar to `alpha`, this column represents the declination of the object, which is also a coordinate for locating objects thus it shouldn't provide any benefit to the model.\n",
    "- `run_ID`: this feature refers to a specific scan of the sky, which is an identifier for the run that the object was observed in. It shouldn't provide any benefit to the model as it isn't related to the detection of an object as a star, galaxy, or quasar.\n",
    "- `cam_col`: this feature represents the camera column used during the observation. Like the `run_ID`, it is not related to the object's properties thus is less relevant to the classification task.\n",
    "- `field_ID`: this feature represents the field number of the observation. It is also not related to the object's properties so shouldn't be relevant to the classification task.\n",
    "- `fiber_ID`: this refers to the fiber number used to capture the spectrum of the object. The feature is not related to the object's properties so won't provide any benefit to the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop the features as described above\n",
    "star_features_selected = np.delete(star_features_standardised, [0, 1, 2, 8, 9, 10, 15], axis=1)\n",
    "\n",
    "# Print the number of features\n",
    "print(f'Number of features: {star_features_selected.shape[1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pearson_correlation_matrix(star_features_selected)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this matrix, it is clear that the following features are highly correlated:\n",
    "- 0 (`u`), 1 (`g`), 2 (`r`), 3 (`i`) and 4 (`z`) have a chain of correlations\n",
    "- 5 (`spec_obj_ID`), 7 (`plate`) and 8 (`MJD`) have linear correlations\n",
    "\n",
    "It makes sense that several of the features related to the photometric system are correlated as they represent the intensity of the object's light at different wavelengths. Since objects emit light across a range of wavelengths, it is expected that the light will be correlated across the different filters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GWP Dataset Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the GWP dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gwp_file_name = 'datasets/gwp_assessment.csv'\n",
    "gwp_categorical = np.genfromtxt(gwp_file_name, delimiter=',', skip_header=True, encoding=\"utf-8\", dtype=str, usecols=range(0, 4))\n",
    "gwp_numerical = np.genfromtxt(gwp_file_name, delimiter=',', skip_header=True, encoding=\"utf-8\", dtype=np.float64, usecols=range(4, 14))\n",
    "gwp_values = np.genfromtxt(gwp_file_name, delimiter=',', skip_header=True, encoding=\"utf-8\", dtype=np.float64, usecols=14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Explore the gwp dataset\n",
    "print(f'Number of samples: {gwp_categorical.shape[0]}')\n",
    "print(f'Number of features: {gwp_categorical.shape[1] + gwp_numerical.shape[1]}')\n",
    "print(f'Number of classes: {np.unique(gwp_values).shape[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there are 879 classes, this is clearly a regression task. Additionally, unlike the star dataset, since there are categorical features in this dataset, these will need to be explored and encoded appropriately"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explore Categorical Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First let's look at `quarter`, `date` and `day`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_quarter_values, counts = np.unique(gwp_categorical[:, 1], return_counts=True)\n",
    "\n",
    "# Print unique values along with their counts\n",
    "for value, count in zip(unique_quarter_values, counts):\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the dates against each quarter\n",
    "plt.figure(figsize=(20, 5))\n",
    "gwp_categorical_no_nulls = gwp_categorical[~(gwp_categorical == '').any(axis=1)]\n",
    "plt.plot(gwp_categorical_no_nulls[:, 0], gwp_categorical_no_nulls[:, 1])\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Quarter')\n",
    "plt.title('Dates vs. Quarter')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the plot above, it is clear that quarters 1, 2, 3 and 4 usually follow a consistent pattern in their length (one week) and when they occur (start every Thursday). The main point of deviation from this pattern is quarter 5, which happens once and appears to last for only two days."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's determine whether any samples are missing more than one of the date, day and quarter features\n",
    "missing_samples = np.column_stack((gwp_categorical[:, 0] == '', gwp_categorical[:, 1] == '', gwp_categorical[:, 3] == ''))\n",
    "missing_count = np.sum(missing_samples, axis=1)\n",
    "rows_with_multiple_missing = np.where(missing_count > 1)[0]\n",
    "\n",
    "print(f\"Number of rows with more than one missing value: {rows_with_multiple_missing.shape[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since no rows are missing more than one of the date, day and quarter features, the missing value can be inferred from the remaining features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gwp_categorical_filled = fill_missing_dates(gwp_categorical)\n",
    "gwp_categorical_filled = fill_missing_quarters(gwp_categorical_filled)\n",
    "gwp_categorical_filled = fill_missing_days(gwp_categorical_filled)\n",
    "\n",
    "# Print the number of missing values in these features\n",
    "print(f\"Number of missing values in date column: {np.sum(gwp_categorical_filled[:, 0] == '')}\")\n",
    "print(f\"Number of missing values in quarter column: {np.sum(gwp_categorical_filled[:, 1] == '')}\")\n",
    "print(f\"Number of missing values in day column: {np.sum(gwp_categorical_filled[:, 3] == '')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Next let's look at the `department` categorical feature\n",
    "unique_values, counts = np.unique(gwp_categorical_filled[:, 2], return_counts=True)\n",
    "\n",
    "# Print unique values along with their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f'Value: {value}, Count: {count}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There appears to be two departments, `sweing` and `finishing`. However, some `finishing` values have been entered with a trailing space, which will need to be removed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove the trailing space from the `finishing` values\n",
    "gwp_categorical_filled[:, 2] = np.char.strip(gwp_categorical_filled[:, 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that there are only seven samples missing the `department` feature let's drop those samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop the samples missing the `department` feature\n",
    "mask = gwp_categorical_filled[:, 2] != ''\n",
    "gwp_categorical_filled = gwp_categorical_filled[mask]\n",
    "gwp_numerical = gwp_numerical[mask]\n",
    "gwp_values = gwp_values[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Encode Categorical Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First let's convert the `date` feature into several numerical features: `year`, `month` and `day`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gwp_categorical_date_encoded = convert_date_to_cols(gwp_categorical_filled, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use one-hot encoding to encode the `quarter`, `department` and `day` features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a copy of the first three columns of gwp_categorical_date_encoded\n",
    "gwp_categorical_ohc = gwp_categorical_date_encoded[:, :3].copy()\n",
    "\n",
    "for i in range(3, 6):\n",
    "    ohc = OneHotEncoder(categories='auto', dtype=float, sparse_output=False)\n",
    "    new_col = ohc.fit_transform(gwp_categorical_date_encoded[:, [i]])\n",
    "    gwp_categorical_ohc = np.hstack((gwp_categorical_ohc, new_col))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Numerical Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the number of missing values in each column\n",
    "missing_numerical_vals = np.sum(np.isnan(gwp_numerical), axis=0)\n",
    "for k, v in enumerate(missing_numerical_vals):\n",
    "    print(f'Column {k+1}: {v} missing values')\n",
    "print(f'Total: {sum(missing_numerical_vals)} missing values')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this it is clear that the `wip` feature has many missing values. Therefore, this feature needs to be investigated further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a bar chart of the `wip` feature for each department\n",
    "department = np.unique(gwp_categorical_filled[:, 2])\n",
    "# Count the number of non-null values in the `wip` feature for each department\n",
    "wip = np.zeros(department.shape[0])\n",
    "for i, dept in enumerate(department):\n",
    "    wip[i] = np.sum(~np.isnan(gwp_numerical[gwp_categorical_filled[:, 2] == dept, 3]))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(department, wip)\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('WIP')\n",
    "plt.title('WIP for each department')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the plot above, it is clear that the `wip` feature is missing for all samples in the `finishing` department. Based on the definition of the `wip` feature (work in progress, includes the number of unfinished items for products), it appears to be not applicable to the `finishing` department. Therefore, we can just fill in the missing values with zeros where the department is `finishing`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fill in the missing values in the `wip` feature with zeros when the department is `finishing`\n",
    "gwp_numerical[(gwp_categorical_filled[:, 2] == 'finishing') & (np.isnan(gwp_numerical[:, 3])), 3] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the remaining missing values, we can use a KNN imputer to fill in the missing values. However, first let's combine the categorical and numerical features into a single numpy array."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gwp_features = np.hstack((gwp_categorical_ohc, gwp_numerical))\n",
    "\n",
    "knn_imputer = KNNImputer()\n",
    "gwp_features_imputed = knn_imputer.fit_transform(gwp_features)\n",
    "\n",
    "missing_vals = np.sum(np.isnan(gwp_features_imputed), axis=0)\n",
    "print(f'Total: {sum(missing_vals)} missing values')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's one-hot encode the `team` feature as it can be considered a categorical feature. Before this, we need to round the values in the `team` feature to the nearest integer as they may have been imputed with decimal values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Round the values in the `team` feature to the nearest integer\n",
    "gwp_features_imputed[:, 16] = np.round(gwp_features_imputed[:, 16])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-hot encode the `team` feature then replace the original `team` feature with the encoded features\n",
    "ohc = OneHotEncoder(categories='auto', dtype=float, sparse_output=False)\n",
    "team = ohc.fit_transform(gwp_features_imputed[:, [16]])\n",
    "gwp_features_encoded = np.hstack((gwp_features_imputed[:, :16], team, gwp_features_imputed[:, 17:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove Outliers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "continuous_features = list(range(0, 28)) + list(range(33, 36))\n",
    "gwp_features_without_outliers, gwp_values_without_outliers = remove_outliers(gwp_features_encoded, gwp_values,\n",
    "                                                                             ignore_cols=continuous_features)\n",
    "print(f'Number of samples removed: {gwp_features_encoded.shape[0] - gwp_features_without_outliers.shape[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove Features with No Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gwp_features_no_variance_removed, removed_columns = remove_no_variance(gwp_features_without_outliers)\n",
    "\n",
    "print(f\"Removed column indexes: {removed_columns}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Standardise Features and Target Variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gwp_features_standardised = standardise_features(gwp_features_no_variance_removed, cols=range(27, 36))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For performing feature selection on this dataset, we will use the Pearson correlation coefficient as the target variable is continuous. A threshold of 0.1 will be used as a guideline to select the features, along with knowledge gained through looking at the definition of each feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "select_k_best = SelectKBest(r_regression)\n",
    "select_k_best.fit(gwp_features_standardised, gwp_values_without_outliers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the scores on a line graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(select_k_best.scores_, marker='o')\n",
    "plt.axhline(y=0.1, color='g', linestyle='--')\n",
    "plt.axhline(y=-0.1, color='g', linestyle='--')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Pearson Correlation Coefficient for each Feature')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the plot above, we can select the following features:\n",
    "- `team` (one-hot encoded): despite all the one-hot encoded `team` features not having a correlation coefficient greater than 0.1, we will keep all of them as they are all related to the same feature.\n",
    "- `targeted_productivity`: this feature has the highest correlation coefficient.\n",
    "- `smv`: this feature has a correlation coefficient less than -0.1, and it makes sense that the standard minute value of a task would affect the productivity.\n",
    "- `idle_men`: this feature also has a correlation coefficient less than -0.1, and relates to the number of workers that were idle due to production interruption.\n",
    "- `no_of_style_change`: this feature has a correlation coefficient less than -0.1, and represents the number of times the style of a product was changed during production."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop the remaining features\n",
    "gwp_features_selected = np.delete(gwp_features_standardised, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                                                               11, 12, 13, 14, 29, 30, 31, 32, 33], axis=1)\n",
    "\n",
    "# Print the number of features\n",
    "print(f'Number of features: {gwp_features_selected.shape[1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pearson_correlation_matrix(gwp_features_selected, show_coef=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The matrix shows that most of the selected features are not strongly correlated with each other, except for `smv` and `no_of_style_change`. However, since generally the features are not strongly correlated, we will keep all of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Markdown Question"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Explain the logic behind usage of Pearson correlation coefficient for feature selection.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the Pearson correlation coefficient for feature selection is suitable when the target variable is continuous because it is a measure of the linear relationship between two variables. It can be used to measure the strength and correlation between each feature and the target variable to determine which have the greatest impact. The result for each feature is a value between -1 and 1, where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation. Therefore, it is relatively simple to interpret the relationship between features and the target. It is common to set a threshold for coefficient values based on domain knowledge to determine which features to select. The Pearson correlation coefficient can also be used to identify features that have a high level of correlation with each other, with the aim of reducing multicollinearity.\n",
    "\n",
    "In the context of feature selection in the two datasets pre-processed in this notebook, it was used to identify which features are most relevant to the target variable for the GWP dataset as the target was continuous. In contrast, it was not used for the STAR dataset as the target was categorical, but instead for examining the relationship between continuous features."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
